{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1ë‹¨ê³„: í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain-community langchain-text-splitters voyageai pymongo ipywidgets boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import voyageai\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB ì—°ê²° ë¬¸ìì—´ì„ ë„£ìŠµë‹ˆë‹¤.\n",
    "MONGODB_URI = \"\"\n",
    "# Voyage AI API Keyë¥¼ ë„£ìŠµë‹ˆë‹¤.\n",
    "VOYAGE_API_KEY = \"\"\n",
    "\n",
    "MONGODB_URI = \"\"\n",
    "VOYAGE_API_KEY = \"\"\n",
    "BEDROCK_API_KEY= \"\"\n",
    "# 2. ëŸ°íƒ€ì„ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = BEDROCK_API_KEY\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"ap-northeast-2\"\n",
    ")\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„\n",
    "DB_NAME = \"mdb_ai_workshop\"\n",
    "\n",
    "# MongoDB Python í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "mongodb_client = MongoClient(MONGODB_URI)\n",
    "# ì„œë²„ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "mongodb_client.admin.command(\"ping\")\n",
    "\n",
    "# Voyage AI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "vo = voyageai.Client(api_key=VOYAGE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.collection import Collection\n",
    "from pymongo.errors import OperationFailure\n",
    "from typing import List, Dict, Optional\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import time\n",
    "\n",
    "SLEEP_TIMER = 3\n",
    "\n",
    "def create_index(collection: Collection, index_name: str, model: Dict) -> None:\n",
    "    try:\n",
    "        print(f\"Creating the {index_name} index\")\n",
    "        collection.create_search_index(model=model)\n",
    "    except OperationFailure:\n",
    "        print(f\"{index_name} index already exists, recreating...\")\n",
    "        try:\n",
    "            print(f\"Dropping {index_name} index\")\n",
    "            collection.drop_search_index(name=index_name)\n",
    "\n",
    "            # Poll for index deletion to complete\n",
    "            while True:\n",
    "                indexes = list(collection.list_search_indexes())\n",
    "                index_exists = any(idx.get(\"name\") == index_name for idx in indexes)\n",
    "                if not index_exists:\n",
    "                    print(f\"{index_name} index deletion complete\")\n",
    "                    break\n",
    "                print(f\"Waiting for {index_name} index deletion to complete...\")\n",
    "                time.sleep(SLEEP_TIMER)\n",
    "\n",
    "            print(f\"Creating new {index_name} index\")\n",
    "            collection.create_search_index(model=model)\n",
    "            print(f\"Successfully recreated the {index_name} index\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error during index recreation: {str(e)}\")\n",
    "\n",
    "\n",
    "def check_index_ready(collection: Collection, index_name: str) -> None:\n",
    "    while True:\n",
    "        indexes = list(collection.list_search_indexes())\n",
    "        matching_indexes = [idx for idx in indexes if idx.get(\"name\") == index_name]\n",
    "\n",
    "        if not matching_indexes:\n",
    "            print(f\"{index_name} index not found\")\n",
    "            time.sleep(SLEEP_TIMER)\n",
    "            continue\n",
    "\n",
    "        index = matching_indexes[0]\n",
    "        status = index[\"status\"]\n",
    "        if status == \"READY\":\n",
    "            print(f\"{index_name} index status: READY\")\n",
    "            print(f\"{index_name} index definition: {index['latestDefinition']}\")\n",
    "            break\n",
    "\n",
    "        print(f\"{index_name} index status: {status}\")\n",
    "        time.sleep(SLEEP_TIMER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2ë‹¨ê³„: ë°ì´í„°ì…‹ ë¡œë“œí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"assets/datas/mongodb_docs.json\", \"r\") as data_file:\n",
    "    json_data = data_file.read()\n",
    "\n",
    "docs = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ì— í¬í•¨ëœ ë¬¸ì„œì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ í•˜ë‚˜ë¥¼ ë¯¸ë¦¬ ë´…ë‹ˆë‹¤.\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3ë‹¨ê³„: ë°ì´í„° ì²­í‚¹(Chunking) ë° ì„ë² ë”©(Embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë°ì´í„° ì²­í‚¹ì‹œ ê³ ë ¤í•´ì•¼í•  ë¶€ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì²­í‚¹ì‹œ ê³ ë ¤í•´ì•¼í•  ë¶€ë¶„ë“¤ì…ë‹ˆë‹¤.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0, # ë¹„êµë¥¼ ìœ„í•´ ì˜¤ë²„ë©ì„ 0ìœ¼ë¡œ ë‘¡ë‹ˆë‹¤. (ì˜¤ë²„ë©ì´ ìˆì–´ë„ ë¬¸ì œëŠ” ë°œìƒí•©ë‹ˆë‹¤)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"]\n",
    ")\n",
    "\n",
    "standard_chunks = []\n",
    "\n",
    "for doc in docs:\n",
    "    # ë¬¸ì„œì˜ ë³¸ë¬¸(body)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    original_text = doc.get(\"body\", \"\")\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ë¶„í•  ìˆ˜í–‰\n",
    "    splits = text_splitter.split_text(original_text)\n",
    "    \n",
    "    for i, split_text in enumerate(splits):\n",
    "        standard_chunks.append({\n",
    "            \"source_title\": doc.get(\"title\"),\n",
    "            \"chunk_index\": i,\n",
    "            \"text\": split_text\n",
    "        })\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸ (ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥)\n",
    "for chunk in standard_chunks[:5]:\n",
    "    print(f\"--- Chunk {chunk['chunk_index']} ({chunk['source_title']}) ---\")\n",
    "    print(chunk['text'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“š https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChainì˜ `RecursiveCharacterTextSplitter`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¨¼ì € `separators` ëª©ë¡ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "# `model_name` ë§¤ê°œë³€ìˆ˜ëŠ” í† í°í™”ì— ì‚¬ìš©í•  ì¸ì½”ë”ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” GPT-4ì˜ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\", \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"], \n",
    "    chunk_size=200, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "def get_chunks(doc: Dict, text_field: str) -> List[Dict]:\n",
    "    # `doc`ì—ì„œ ì²­í‚¹í•  í•„ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    text = doc[text_field]\n",
    "    # ìœ„ì—ì„œ ì •ì˜í•œ `text_splitter` ê°ì²´ì˜ `split_text` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ `text`ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“š https://docs.voyageai.com/docs/contextualized-chunk-embeddings#approach-2-contextualized-chunk-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(content: List[str], input_type: str) -> List[float] | List[List[float]]:\n",
    "    # Voyage AI APIì˜ `contextualized_embed` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì²­í¬ì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    # inputs: ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¼ `content`\n",
    "    # model: `voyage-context-3`\n",
    "    # input_type: `input_type` ì¸ìê°’\n",
    "    embds_obj = vo.contextualized_embed(inputs=[content], model=\"voyage-context-3\", input_type=input_type)\n",
    "    if input_type == \"document\":\n",
    "        embeddings = [emb for r in embds_obj.results for emb in r.embeddings]\n",
    "    if input_type == \"query\":\n",
    "        embeddings = embds_obj.results[0].embeddings[0]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = []\n",
    "# 2ë‹¨ê³„ì˜ `docs`ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤.\n",
    "for doc in tqdm(docs):\n",
    "    # `get_chunks` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì„œì˜ \"body\" í•„ë“œë¥¼ ì²­í‚¹í•©ë‹ˆë‹¤.\n",
    "    chunks = get_chunks(doc, \"body\")\n",
    "    # ëª¨ë“  `chunks`ë¥¼ `get_embeddings` í•¨ìˆ˜ì— ì „ë‹¬í•˜ì—¬ ê° ì²­í¬ì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    # RAGë¥¼ ìœ„í•œ \"ë¬¸ì„œ\"ë¥¼ ì„ë² ë”©í•˜ë¯€ë¡œ `input_type`ì€ \"document\"ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    chunk_embeddings = get_embeddings(chunks, \"document\")\n",
    "    # ê° ì²­í¬ì— ëŒ€í•´ ì›ë³¸ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì§„ ìƒˆë¡œìš´ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    # `body`ë¥¼ ì²­í¬ ë‚´ìš©ìœ¼ë¡œ êµì²´í•˜ê³  `embedding` í•„ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    for chunk, embedding in zip(chunks, chunk_embeddings):\n",
    "        # ì›ë³¸ ë¬¸ì„œë¥¼ ë³µì‚¬í•˜ì—¬ ìƒˆ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        chunk_doc = doc.copy()\n",
    "        # `chunk_doc`ì˜ `body` í•„ë“œë¥¼ ì²­í¬ ë‚´ìš©ìœ¼ë¡œ êµì²´í•©ë‹ˆë‹¤.\n",
    "        chunk_doc[\"body\"] = chunk\n",
    "        # ì´ ì²­í¬ì˜ ì„ë² ë”© ê°’ì„ ë‹´ì€ `embedding` í•„ë“œë¥¼ `chunk_doc`ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        chunk_doc[\"embedding\"] = embedding\n",
    "        # `chunk_doc`ì„ `embedded_docs` ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        embedded_docs.append(chunk_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `embedded_docs`ì˜ ê¸¸ì´ê°€ 2ë‹¨ê³„ì˜ `docs` ê¸¸ì´ë³´ë‹¤ í° ê²ƒì„ í™•ì¸í•˜ì„¸ìš”.\n",
    "# ì´ëŠ” `docs`ì˜ ê° ë¬¸ì„œê°€ ì—¬ëŸ¬ ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "len(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²­í‚¹ëœ ë¬¸ì„œì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ í•˜ë‚˜ë¥¼ ë¯¸ë¦¬ ë´…ë‹ˆë‹¤.\n",
    "# ì›ë³¸ ë¬¸ì„œì™€ êµ¬ì¡°ê°€ ë¹„ìŠ·í•´ ë³´ì´ì§€ë§Œ, `body` í•„ë“œì— ë” ì‘ì€ í…ìŠ¤íŠ¸ ì¡°ê°ì´ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.\n",
    "# ë˜í•œ ê° ë¬¸ì„œì— `embedding` í•„ë“œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "embedded_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chunk_sizeë¥¼ ê°™ê²Œ í–ˆëŠ”ë° ë‘ë²ˆì§¸ì— ë” ê¸´ ê²°ê³¼ë¬¼ì´ ë‚˜ì˜¨ ì´ìœ \n",
    "```python\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200 # ê¸€ì ìˆ˜ ê¸°ì¤€\n",
    "```\n",
    "\n",
    "```python\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=200 # í† í° ìˆ˜ ê¸°ì¤€ (GPT-4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4ë‹¨ê³„: MongoDBë¡œ ë°ì´í„° ì…ë ¥(Ingest)í•˜ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"knowledge_base\"\n",
    "VECTOR_SEARCH_INDEX_NAME = f\"{DB_NAME}_rag\"\n",
    "\n",
    "collection = mongodb_client[DB_NAME][COLLECTION_NAME]\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“š https://pymongo.readthedocs.io/en/stable/examples/bulk.html#bulk-insert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `embedded_docs`ë¥¼ ìœ„ì—ì„œ ì •ì˜í•œ `collection`ì— ì¼ê´„ ì…ë ¥(bulk insert)í•©ë‹ˆë‹¤. -- í•œ ì¤„ì˜ ì½”ë“œë¡œ ì‹¤í–‰í•˜ì„¸ìš”.\n",
    "collection.insert_many(embedded_docs)\n",
    "\n",
    "print(f\"{collection.count_documents({})}ê°œì˜ ë¬¸ì„œê°€ {COLLECTION_NAME} ì»¬ë ‰ì…˜ì— ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¡í„° ì¸ë±ìŠ¤ ì •ì˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë‹¤ìŒ í•­ëª©ë“¤ì„ ì§€ì •í•˜ì„¸ìš”:\n",
    "# path: ì„ë² ë”© í•„ë“œì˜ ê²½ë¡œ\n",
    "# numDimensions: ì„ë² ë”© ì°¨ì› ìˆ˜ (ì‚¬ìš©ëœ ì„ë² ë”© ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„)\n",
    "# similarity: ìœ ì‚¬ë„ ì¸¡ì • ë°©ì‹ (cosine, euclidean, dotProduct ì¤‘ í•˜ë‚˜)\n",
    "model = {\n",
    "    \"name\": VECTOR_SEARCH_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 1024,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            {\"type\": \"filter\", \"path\": \"metadata.contentType\"},\n",
    "            {\"type\": \"filter\", \"path\": \"updated\"}\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `create_index` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ `collection` ì»¬ë ‰ì…˜ì— ìœ„ ì •ì˜ëŒ€ë¡œ ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "create_index(collection, VECTOR_SEARCH_INDEX_NAME, model)\n",
    "\n",
    "# ì§„í–‰í•˜ê¸° ì „ì— `check_index_ready` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ë±ìŠ¤ê°€ ìƒì„±ë˜ì—ˆê³  'READY' ìƒíƒœì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "check_index_ready(collection, VECTOR_SEARCH_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6ë‹¨ê³„: ë°ì´í„°ì— ëŒ€í•´ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰í•˜ê¸°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë²¡í„° ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "ğŸ“š https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (\"Basic Example\" ì°¸ê³ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¡í„° ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def vector_search(user_query: str, content_type: Optional[str] = None, updated: Optional[str] = None) -> List[Dict]:\n",
    "    query_embedding = get_embeddings([user_query], \"query\")\n",
    "\n",
    "    # 2. ë™ì  í•„í„° ì¡°ê±´ ìƒì„±\n",
    "    filter_conditions = []\n",
    "\n",
    "    if content_type:\n",
    "        filter_conditions.append({\"metadata.contentType\": content_type})\n",
    "    \n",
    "    if updated:\n",
    "        filter_conditions.append({\"updated\": {\"$gte\": updated}})\n",
    "\n",
    "    # 3. $vectorSearch ìŠ¤í…Œì´ì§€ ê¸°ë³¸ êµ¬ì„±\n",
    "    vector_search_stage = {\n",
    "        \"index\": VECTOR_SEARCH_INDEX_NAME,\n",
    "        \"path\": \"embedding\",\n",
    "        \"queryVector\": query_embedding,\n",
    "        \"numCandidates\": 150,\n",
    "        \"limit\": 20\n",
    "    }\n",
    "\n",
    "    if len(filter_conditions) > 1:\n",
    "        vector_search_stage[\"filter\"] = {\"$and\": filter_conditions}\n",
    "    elif len(filter_conditions) == 1:\n",
    "        vector_search_stage[\"filter\"] = filter_conditions[0]\n",
    "    \n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": vector_search_stage\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"body\": 1,\n",
    "                \"metadata.productName\": 1, \n",
    "                \"metadata.contentType\": 1,\n",
    "                \"updated\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7ë‹¨ê³„: RAG ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•í•˜ê¸°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì±„íŒ… í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    # `vector_search` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ `user_query`ì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "    vector_search_result = vector_search(user_query)\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤. ê° ë¬¸ì„œëŠ” ë‘ ê°œì˜ ì¤„ ë°”ê¿ˆ(\"\\n\\n\")ìœ¼ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.\n",
    "    context = \"\\n\\n\".join([doc.get('body') for doc in vector_search_result])\n",
    "    # ì§ˆë¬¸ê³¼ ë‹µë³€ì— í•„ìš”í•œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‚¬ìš©ì ì¿¼ë¦¬ì— ë‹µë³€í•˜ëŠ” í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©ì ì¿¼ë¦¬ì— ë‹µë³€í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "def generate_answer(user_query: str) -> None:\n",
    "    # ìœ„ì˜ `create_prompt` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì±„íŒ… í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    prompt = create_prompt(user_query)\n",
    "    # ì±„íŒ… ë©”ì‹œì§€ë¥¼ AI ëª¨ë¸ í”„ë¡ì‹œë¡œ ì „ì†¡í•˜ì—¬ LLM ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.\n",
    "\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    \n",
    "    payload = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0, \n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        # 3. ëª¨ë¸ í˜¸ì¶œ\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        # 4. ì‘ë‹µ íŒŒì‹±\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        answer = result[\"content\"][0][\"text\"]\n",
    "        print(answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Bedrock: {e}\")\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG ì• í”Œë¦¬ì¼€ì´ì…˜ ì¿¼ë¦¬í•˜ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ ë‹¨ê³„ì—ì„œëŠ” LLMì´ ëŒ€í™” ê¸°ë¡ì„ ê¸°ì–µí•˜ì§€ ëª»í•œë‹¤ëŠ” ì ì„ ì£¼ëª©í•˜ì„¸ìš”.\n",
    "generate_answer(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”(Re-rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“š https://docs.voyageai.com/docs/reranker#python-api (ì˜ˆì œ ì°¸ê³ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ì— ì¬ìˆœìœ„í™”(re-ranking) ë‹¨ê³„ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "def create_prompt_with_rerank(user_query: str) -> str:\n",
    "    context = vector_search(user_query)\n",
    "    documents = [d.get(\"body\") for d in context]\n",
    "\n",
    "    # Voyage AI APIì˜ `rerank` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ `documents`ë¥¼ ì¬ìˆœìœ„í™”í•©ë‹ˆë‹¤.\n",
    "    reranked_documents = vo.rerank(user_query, documents, model=\"rerank-2.5\", top_k=3)\n",
    "    \n",
    "    # ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤. ê° ë¬¸ì„œëŠ” ë‘ ê°œì˜ ì¤„ ë°”ê¿ˆ(\"\\n\\n\")ìœ¼ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.\n",
    "    context = \"\\n\\n\".join([d.document for d in reranked_documents.results])\n",
    "    \n",
    "    # ì§ˆë¬¸ê³¼ ë‹µë³€ì— í•„ìš”í•œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt\n",
    "\n",
    "def generate_answer_with_rerank(user_query: str) -> None:\n",
    "    # ìœ„ì˜ `create_prompt` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì±„íŒ… í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    prompt = create_prompt_with_rerank(user_query)\n",
    "    # ì±„íŒ… ë©”ì‹œì§€ë¥¼ AI ëª¨ë¸ í”„ë¡ì‹œë¡œ ì „ì†¡í•˜ì—¬ LLM ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.\n",
    "\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    \n",
    "    payload = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0, \n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        # 3. ëª¨ë¸ í˜¸ì¶œ\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        # 4. ì‘ë‹µ íŒŒì‹±\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        answer = result[\"content\"][0][\"text\"]\n",
    "        print(answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Bedrock: {e}\")\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\"My CPU steal metric is high. What should I do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer_with_rerank(\"My CPU steal metric is high. What should I do?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
