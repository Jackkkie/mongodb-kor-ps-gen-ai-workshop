{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1단계: 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain-community langchain-text-splitters voyageai pymongo ipywidgets boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import voyageai\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB 연결 문자열을 넣습니다.\n",
    "MONGODB_URI = \"\"\n",
    "# Voyage AI API Key를 넣습니다.\n",
    "VOYAGE_API_KEY = \"\"\n",
    "\n",
    "MONGODB_URI = \"\"\n",
    "VOYAGE_API_KEY = \"\"\n",
    "BEDROCK_API_KEY= \"\"\n",
    "# 2. 런타임 클라이언트 초기화\n",
    "os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = BEDROCK_API_KEY\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"ap-northeast-2\"\n",
    ")\n",
    "\n",
    "# 데이터베이스 이름\n",
    "DB_NAME = \"mdb_ai_workshop\"\n",
    "\n",
    "# MongoDB Python 클라이언트를 초기화합니다.\n",
    "mongodb_client = MongoClient(MONGODB_URI)\n",
    "# 서버 연결 상태를 확인합니다.\n",
    "mongodb_client.admin.command(\"ping\")\n",
    "\n",
    "# Voyage AI 클라이언트를 초기화합니다.\n",
    "vo = voyageai.Client(api_key=VOYAGE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 유틸리티 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.collection import Collection\n",
    "from pymongo.errors import OperationFailure\n",
    "from typing import List, Dict, Optional\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import time\n",
    "\n",
    "SLEEP_TIMER = 3\n",
    "\n",
    "def create_index(collection: Collection, index_name: str, model: Dict) -> None:\n",
    "    try:\n",
    "        print(f\"Creating the {index_name} index\")\n",
    "        collection.create_search_index(model=model)\n",
    "    except OperationFailure:\n",
    "        print(f\"{index_name} index already exists, recreating...\")\n",
    "        try:\n",
    "            print(f\"Dropping {index_name} index\")\n",
    "            collection.drop_search_index(name=index_name)\n",
    "\n",
    "            # Poll for index deletion to complete\n",
    "            while True:\n",
    "                indexes = list(collection.list_search_indexes())\n",
    "                index_exists = any(idx.get(\"name\") == index_name for idx in indexes)\n",
    "                if not index_exists:\n",
    "                    print(f\"{index_name} index deletion complete\")\n",
    "                    break\n",
    "                print(f\"Waiting for {index_name} index deletion to complete...\")\n",
    "                time.sleep(SLEEP_TIMER)\n",
    "\n",
    "            print(f\"Creating new {index_name} index\")\n",
    "            collection.create_search_index(model=model)\n",
    "            print(f\"Successfully recreated the {index_name} index\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error during index recreation: {str(e)}\")\n",
    "\n",
    "\n",
    "def check_index_ready(collection: Collection, index_name: str) -> None:\n",
    "    while True:\n",
    "        indexes = list(collection.list_search_indexes())\n",
    "        matching_indexes = [idx for idx in indexes if idx.get(\"name\") == index_name]\n",
    "\n",
    "        if not matching_indexes:\n",
    "            print(f\"{index_name} index not found\")\n",
    "            time.sleep(SLEEP_TIMER)\n",
    "            continue\n",
    "\n",
    "        index = matching_indexes[0]\n",
    "        status = index[\"status\"]\n",
    "        if status == \"READY\":\n",
    "            print(f\"{index_name} index status: READY\")\n",
    "            print(f\"{index_name} index definition: {index['latestDefinition']}\")\n",
    "            break\n",
    "\n",
    "        print(f\"{index_name} index status: {status}\")\n",
    "        time.sleep(SLEEP_TIMER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2단계: 데이터셋 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"assets/datas/mongodb_docs.json\", \"r\") as data_file:\n",
    "    json_data = data_file.read()\n",
    "\n",
    "docs = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에 포함된 문서의 개수를 확인합니다.\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 구조를 파악하기 위해 하나를 미리 봅니다.\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3단계: 데이터 청킹(Chunking) 및 임베딩(Embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 청킹시 고려해야할 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 청킹시 고려해야할 부분들입니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0, # 비교를 위해 오버랩을 0으로 둡니다. (오버랩이 있어도 문제는 발생합니다)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"]\n",
    ")\n",
    "\n",
    "standard_chunks = []\n",
    "\n",
    "for doc in docs:\n",
    "    # 문서의 본문(body)을 가져옵니다.\n",
    "    original_text = doc.get(\"body\", \"\")\n",
    "    \n",
    "    # 텍스트 분할 수행\n",
    "    splits = text_splitter.split_text(original_text)\n",
    "    \n",
    "    for i, split_text in enumerate(splits):\n",
    "        standard_chunks.append({\n",
    "            \"source_title\": doc.get(\"title\"),\n",
    "            \"chunk_index\": i,\n",
    "            \"text\": split_text\n",
    "        })\n",
    "\n",
    "# 결과 확인 (처음 5개만 출력)\n",
    "for chunk in standard_chunks[:5]:\n",
    "    print(f\"--- Chunk {chunk['chunk_index']} ({chunk['source_title']}) ---\")\n",
    "    print(chunk['text'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain의 `RecursiveCharacterTextSplitter`를 사용하여 먼저 `separators` 목록을 기준으로 텍스트를 분할합니다.\n",
    "# `model_name` 매개변수는 토큰화에 사용할 인코더를 지정합니다. 여기서는 GPT-4의 인코더를 사용합니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\", \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"], \n",
    "    chunk_size=200, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "def get_chunks(doc: Dict, text_field: str) -> List[Dict]:\n",
    "    # `doc`에서 청킹할 필드를 추출합니다.\n",
    "    text = doc[text_field]\n",
    "    # 위에서 정의한 `text_splitter` 객체의 `split_text` 메서드를 사용하여 `text`를 분할합니다.\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.voyageai.com/docs/contextualized-chunk-embeddings#approach-2-contextualized-chunk-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(content: List[str], input_type: str) -> List[float] | List[List[float]]:\n",
    "    # Voyage AI API의 `contextualized_embed` 메서드를 사용하여 각 청크의 임베딩을 생성합니다.\n",
    "    # inputs: 리스트로 감싼 `content`\n",
    "    # model: `voyage-context-3`\n",
    "    # input_type: `input_type` 인자값\n",
    "    embds_obj = vo.contextualized_embed(inputs=[content], model=\"voyage-context-3\", input_type=input_type)\n",
    "    if input_type == \"document\":\n",
    "        embeddings = [emb for r in embds_obj.results for emb in r.embeddings]\n",
    "    if input_type == \"query\":\n",
    "        embeddings = embds_obj.results[0].embeddings[0]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = []\n",
    "# 2단계의 `docs`를 순회합니다.\n",
    "for doc in tqdm(docs):\n",
    "    # `get_chunks` 함수를 사용하여 각 문서의 \"body\" 필드를 청킹합니다.\n",
    "    chunks = get_chunks(doc, \"body\")\n",
    "    # 모든 `chunks`를 `get_embeddings` 함수에 전달하여 각 청크의 임베딩을 생성합니다.\n",
    "    # RAG를 위한 \"문서\"를 임베딩하므로 `input_type`은 \"document\"로 설정해야 합니다.\n",
    "    chunk_embeddings = get_embeddings(chunks, \"document\")\n",
    "    # 각 청크에 대해 원본 메타데이터를 가진 새로운 문서를 생성합니다.\n",
    "    # `body`를 청크 내용으로 교체하고 `embedding` 필드를 추가합니다.\n",
    "    for chunk, embedding in zip(chunks, chunk_embeddings):\n",
    "        # 원본 문서를 복사하여 새 문서를 생성합니다.\n",
    "        chunk_doc = doc.copy()\n",
    "        # `chunk_doc`의 `body` 필드를 청크 내용으로 교체합니다.\n",
    "        chunk_doc[\"body\"] = chunk\n",
    "        # 이 청크의 임베딩 값을 담은 `embedding` 필드를 `chunk_doc`에 추가합니다.\n",
    "        chunk_doc[\"embedding\"] = embedding\n",
    "        # `chunk_doc`을 `embedded_docs` 리스트에 추가합니다.\n",
    "        embedded_docs.append(chunk_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `embedded_docs`의 길이가 2단계의 `docs` 길이보다 큰 것을 확인하세요.\n",
    "# 이는 `docs`의 각 문서가 여러 개의 청크로 분할되었기 때문입니다.\n",
    "len(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청킹된 문서의 구조를 파악하기 위해 하나를 미리 봅니다.\n",
    "# 원본 문서와 구조가 비슷해 보이지만, `body` 필드에 더 작은 텍스트 조각이 들어있습니다.\n",
    "# 또한 각 문서에 `embedding` 필드가 추가되었습니다.\n",
    "embedded_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chunk_size를 같게 했는데 두번째에 더 긴 결과물이 나온 이유\n",
    "```python\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200 # 글자 수 기준\n",
    "```\n",
    "\n",
    "```python\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=200 # 토큰 수 기준 (GPT-4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4단계: MongoDB로 데이터 입력(Ingest)하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"knowledge_base\"\n",
    "VECTOR_SEARCH_INDEX_NAME = f\"{DB_NAME}_rag\"\n",
    "\n",
    "collection = mongodb_client[DB_NAME][COLLECTION_NAME]\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pymongo.readthedocs.io/en/stable/examples/bulk.html#bulk-insert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `embedded_docs`를 위에서 정의한 `collection`에 일괄 입력(bulk insert)합니다. -- 한 줄의 코드로 실행하세요.\n",
    "collection.insert_many(embedded_docs)\n",
    "\n",
    "print(f\"{collection.count_documents({})}개의 문서가 {COLLECTION_NAME} 컬렉션에 입력되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5단계: 벡터 검색 인덱스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 인덱스 정의를 생성합니다. 다음 항목들을 지정하세요:\n",
    "# path: 임베딩 필드의 경로\n",
    "# numDimensions: 임베딩 차원 수 (사용된 임베딩 모델에 따라 다름)\n",
    "# similarity: 유사도 측정 방식 (cosine, euclidean, dotProduct 중 하나)\n",
    "model = {\n",
    "    \"name\": VECTOR_SEARCH_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 1024,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            {\"type\": \"filter\", \"path\": \"metadata.contentType\"},\n",
    "            {\"type\": \"filter\", \"path\": \"updated\"}\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `create_index` 함수를 사용하여 `collection` 컬렉션에 위 정의대로 벡터 검색 인덱스를 생성합니다.\n",
    "create_index(collection, VECTOR_SEARCH_INDEX_NAME, model)\n",
    "\n",
    "# 진행하기 전에 `check_index_ready` 함수를 사용하여 인덱스가 생성되었고 'READY' 상태인지 확인합니다.\n",
    "check_index_ready(collection, VECTOR_SEARCH_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6단계: 데이터에 대해 벡터 검색 수행하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터 검색 함수 정의\n",
    "\n",
    "https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (\"Basic Example\" 참고)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 검색을 사용하여 사용자 쿼리와 관련된 문서를 검색하는 함수를 정의합니다.\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def vector_search(user_query: str, content_type: Optional[str] = None, updated: Optional[str] = None) -> List[Dict]:\n",
    "    query_embedding = get_embeddings([user_query], \"query\")\n",
    "\n",
    "    # 2. 동적 필터 조건 생성\n",
    "    filter_conditions = []\n",
    "\n",
    "    if content_type:\n",
    "        filter_conditions.append({\"metadata.contentType\": content_type})\n",
    "    \n",
    "    if updated:\n",
    "        filter_conditions.append({\"updated\": {\"$gte\": updated}})\n",
    "\n",
    "    # 3. $vectorSearch 스테이지 기본 구성\n",
    "    vector_search_stage = {\n",
    "        \"index\": VECTOR_SEARCH_INDEX_NAME,\n",
    "        \"path\": \"embedding\",\n",
    "        \"queryVector\": query_embedding,\n",
    "        \"numCandidates\": 150,\n",
    "        \"limit\": 20\n",
    "    }\n",
    "\n",
    "    if len(filter_conditions) > 1:\n",
    "        vector_search_stage[\"filter\"] = {\"$and\": filter_conditions}\n",
    "    elif len(filter_conditions) == 1:\n",
    "        vector_search_stage[\"filter\"] = filter_conditions[0]\n",
    "    \n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": vector_search_stage\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"body\": 1,\n",
    "                \"metadata.productName\": 1, \n",
    "                \"metadata.contentType\": 1,\n",
    "                \"updated\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 실행 및 결과 반환\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7단계: RAG 애플리케이션 구축하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 채팅 프롬프트 생성 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 애플리케이션을 위한 사용자 프롬프트를 생성하는 함수를 정의합니다.\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    # `vector_search` 함수를 사용하여 `user_query`와 관련된 문서를 검색합니다.\n",
    "    vector_search_result = vector_search(user_query)\n",
    "    # 검색된 문서들을 하나의 문자열로 결합합니다. 각 문서는 두 개의 줄 바꿈(\"\\n\\n\")으로 구분합니다.\n",
    "    context = \"\\n\\n\".join([doc.get('body') for doc in vector_search_result])\n",
    "    # 질문과 답변에 필요한 관련 컨텍스트로 구성된 프롬프트를 만듭니다.\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자 쿼리에 답변하는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 쿼리에 답변하는 함수를 정의합니다.\n",
    "def generate_answer(user_query: str) -> None:\n",
    "    # 위의 `create_prompt` 함수를 사용하여 채팅 프롬프트를 생성합니다.\n",
    "    prompt = create_prompt(user_query)\n",
    "    # 채팅 메시지를 AI 모델 프록시로 전송하여 LLM 응답을 받습니다.\n",
    "\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    \n",
    "    payload = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0, \n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        # 3. 모델 호출\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        # 4. 응답 파싱\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        answer = result[\"content\"][0][\"text\"]\n",
    "        print(answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Bedrock: {e}\")\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG 애플리케이션 쿼리하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 단계에서는 LLM이 대화 기록을 기억하지 못한다는 점을 주목하세요.\n",
    "generate_answer(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검색 결과 재순위화(Re-rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.voyageai.com/docs/reranker#python-api (예제 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수에 재순위화(re-ranking) 단계를 추가합니다.\n",
    "def create_prompt_with_rerank(user_query: str) -> str:\n",
    "    context = vector_search(user_query)\n",
    "    documents = [d.get(\"body\") for d in context]\n",
    "\n",
    "    # Voyage AI API의 `rerank` 메서드를 사용하여 `documents`를 재순위화합니다.\n",
    "    reranked_documents = vo.rerank(user_query, documents, model=\"rerank-2.5\", top_k=3)\n",
    "    \n",
    "    # 재순위화된 문서들을 하나의 문자열로 결합합니다. 각 문서는 두 개의 줄 바꿈(\"\\n\\n\")으로 구분합니다.\n",
    "    context = \"\\n\\n\".join([d.document for d in reranked_documents.results])\n",
    "    \n",
    "    # 질문과 답변에 필요한 관련 컨텍스트로 구성된 프롬프트를 만듭니다.\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt\n",
    "\n",
    "def generate_answer_with_rerank(user_query: str) -> None:\n",
    "    # 위의 `create_prompt` 함수를 사용하여 채팅 프롬프트를 생성합니다.\n",
    "    prompt = create_prompt_with_rerank(user_query)\n",
    "    # 채팅 메시지를 AI 모델 프록시로 전송하여 LLM 응답을 받습니다.\n",
    "\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    \n",
    "    payload = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0, \n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        # 3. 모델 호출\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        # 4. 응답 파싱\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        answer = result[\"content\"][0][\"text\"]\n",
    "        print(answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Bedrock: {e}\")\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\"My CPU steal metric is high. What should I do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer_with_rerank(\"My CPU steal metric is high. What should I do?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
